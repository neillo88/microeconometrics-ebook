# Conditional Expectations

# Causal Regression - Part 1

"We will not have time to discuss **Omitted Variable Bias**. This is an important topic, but is addressed in MHE and MM. I will also provide slides from 2021/22 that go through it."

-   Just as selection is linked to endogeneity,
-   the notion of `selection on observables` is linked to OVB.
-   The standard OVB formula is one way of expressing the problem of selection in a linear model.

"It remains an important topic that is examinable."

## Assignment Mechanism

"This week we discuss discrete (and continuous) treatments that need not arise from a completely random experiment."

Let,

$$
D_i = \mathbf{1}\{treated\}
$$

be an indicator of treatment status.

"In classical randomized experiment the assignment mechanism is known."

### Assignment Mechanisms

"More generally, there are two types of assignment."

**Regular Assignment:** Assignment is (1) individualistic; (2) probabilistic; and (3) unconfounded.

-   In RCTs the functional form of regular assignment is known.
-   In observational studies, it is not. Focus on achieving balance in covariates.

**Irregular Assignment:** Assignment to treatment differs from receipt of treatment.

-   Can have irregular assignment under randomization: estimate ITT or use assignment as instrument (estimate LATE).

-   Typically necessitate additional *exclusion restrictions*.

## Conditional Expectation Function

"In completely randomized experiments, the **observable** difference between conditional expectations provided us with an estimate of the ATE."

$$
E[Y_i|W_i=1] - E[Y_i|W_i=0] = \tau_{\scriptsize{ATT}} = \tau_{\scriptsize{ATE}}
$$

"As the selection component is zero from unconfoundedness."

-   Yet, we have not discussed in detail the **Conditional Expectation Function**,

    $$
    E[Y_i|W_i]
    $$

    "which is integral to our understanding of linear regression."

"The $E[Y_i|X_i]$ is a **random function**"

-   "$X_i$ is a random variable and $E[Y_i|X_i]$ is a function of $X_i$"

"If we fix $X_i=x$, then"

$$
E[Y_i|X_i=x] = \int t \cdot f_y(t|X_i=x)dt = \int t dF_y(t|X_i=x)
$$

"is a **constant** (i.e., not random)."

"Back to our discrete treatment example, $E[Y_i|D_i]$ is a random function, but"

$$
E[Y_i|D_i=1] = E[Y_i(1)|D_i=1] = \int t \cdot f_{y(1)}(t|D_i=1)dt
$$

"and"

$$
E[Y_i|D_i=0] = E[Y_i(0)|D_i=0] = \int t \cdot f_{y(0)}(t|D_i=0)dt
$$

"are both constants."

## Key Points About the CEF

"Here's what you need to know about the CEF:"

1.  "We can express the *observed* outcome $Y_i$ as a sum of $E[Y_i|X_i]+\varepsilon_i$ where $E[\varepsilon_i|X_i]=0$ (i.e., mean independent)."

::: {.callout-caution collapse="true"}
## Proof

1.  $E[\varepsilon_i | X_i] = E[Y_i - E[Y_i | X_i] | X_i] = E[Y_i | X_i] - E[Y_i | X_i] = 0$

2.  $E[h(X_i)\varepsilon_i] = E[h(X_i)E[\varepsilon_i | X_i]] = E[h(X_i) \times 0] = 0$
:::

2.  "$E[Y_i|X_i]$ is the best predictor of $Y_i$."

::: {.callout-caution collapse="true"}
## Proof

\[ (Y_i - m(X_i))\^2 = \left((Y_i - E\[Y_i \| X_i\]) + (E\[Y_i \| X_i\] - m(X_i))\right)\^2 \] \[ = (Y_i - E\[Y_i \| X_i\])\^2 + (E\[Y_i \| X_i\] - m(X_i))\^2 + 2(Y_i - E\[Y_i \| X_i\]) \times (E\[Y_i \| X_i\] - m(X_i)) \]

The last term (cross product) is mean zero. Thus, the function is minimized by setting $m(X_i) = E[Y_i | X_i]$.
:::

3.  "ANOVA Theorem: The variance of $Y_i$ can be decomposed as $V(E[Y_i|X_i])+E(V(Y_i|X_i))$"

::: {.callout-caution collapse="true"}
## Proof
:::

"A second set of theorems related the CEF to the linear population regression function, $X_i'\beta = \beta_1X_{1i} + \beta_2X_{2i} + \ldots + \beta_kX_{ki}$:"

1.  "If $E[Y_i|X_i']$ is linear (in parameters), then the population regression function provides it."
2.  "The function $X_i'\beta$ is the best **linear** predictor of $Y_i$."
3.  "The function $X_i'\beta$ is the best MMSE **linear** approximation of $E[Y_i|X_i]$."

"Does this mean that $X_i'\beta$ is the best predictor of $Y_i$? **NO**, as the $E[Y_i|X_i]$ may not be linear."

"In the Classical Linear Regression Model (CLRM) framework we assume the model is correctly specified as $Y_i = X_i'\beta + \varepsilon_i$ and conditional mean independence $E[\varepsilon_i|X_i'] = 0$. We have assumed that the CEF of $Y_i$ is linear in parameters, $E[Y_i|X_i'] = X_i'\beta$."

"Does this mean that the CEF of $Y^{obs}_i$ always traces out a causal relationship? **NO**, causality depends on the CEFs of $Y_i(0), Y_i(1)$ **and** the assignment mechanism."

# The CEF (of $Y(0)$) Approach

## CEF Approach

The shortcut approach to the identification of causal effects with linear regression involves:

-   **CEF of** $Y_i(0)$: Assume $$
    E[Y_i(0)|X_i'] = X_i'\gamma
    $$ This may include a constant term.

-   **Additive treatment effects**: $$
    Y_i(1) = Y_i(0) + \tau_i
    $$

-   **Heterogeneity independent of** $X$'s: $$
    E[Y_i(1) - Y_i(0)|D_i,X_i'] = E[Y_i(1) - Y_i(0)|D_i]
    $$

We do need to assume an additional assumption concerning assignment:

-   **CIA/Unconfoundedness**: $$
    (Y(1),Y(0)) \perp D | X
    $$

    **or** a weaker,

-   **Unconfoundedness for controls**: $$
    Y(0) \perp D | X
    $$

While we have defined $E[Y_i(0)|X_i']$, the linear regression model will depend on:

$$
\begin{aligned}
E[Y^{obs}_i|D_i,X_i'] &= E[Y_i(0)+\tau_iD_i|D_i,X_i'] \\
                      &= E[Y_i(0)|D_i,X_i']+E[\tau_iD_i|D_i,X_i'] \\
                      &= E[Y_i(0)|X_i']+E[Y_i(1)-Y_i(0)|D_i]D_i \\
                      &= X_i'\gamma + E[Y_i(1)-Y_i(0)|D_i]D_i
\end{aligned}
$$

From line 2 to 3, $$
E[Y_i(0)|D_i,X_i'] = E[Y_i(0)|X_i']
$$ holds if either confoundedness assumptions hold.

If we assume the stronger CIA-unconfoundedness, $$
E[Y_i(1)-Y_i(0)|D_i]D_i = E[Y_i(1)-Y_i(0)]D_i = \tau_{\scriptsize{ATE}}D_i
$$ This gives us the CEF, $$
E[Y^{obs}_i|D_i,X_i'] = \tau_{\scriptsize{ATE}}D_i + X_i'\gamma
$$

If we assume the weaker unconfoundedness for covariates assumption, $$
E[Y_i(1)-Y_i(0)|D_i]D_i = \left(\tau_{ATU} + D_i(\tau_{ATT}-\tau_{ATU})\right)D_i = \tau_{ATT}D_i
$$

This gives us the CEF, $$
E[Y^{obs}_i|D_i,X_i'] = \tau_{ATT}D_i + X_i'\gamma
$$

By specifying the CEF of both $Y_i(1)$ and $Y_i(0)$, we know that $$
Y^{obs}_i = \tau_{ATT}D_i + X_i'\gamma + \varepsilon_i
$$ where, $$
E[\varepsilon_i|D_i,X_i'] = 0
$$

By specifying the CEF of both $Y_i(1)$ and $Y_i(0)$, we understand that:

-   There is no question whether the linear regression model gives us the ATE/ATT.
-   This is a fairly strong assumption and not in the spirit of the RCT approach.
-   When examining RCTs, we were agnostic about the CEF.

## CEF Approach

One crucial omission from this approach is that we do not need to assume overlap (or common support).

::: {custom-style="alertblock"}
**Overlap**: $$
0 < e(X_i) < 1
$$ where, $e(X_i) = Pr(W_i=1|X_i')$
:::

::: {custom-style="exampleblock"}
**Interpretation**: The correctly specified CEF allows us to perfectly predict the unobserved counterfactual of treated individuals based on their $X_i$. With unconfoundedness (for covariates), we can estimate this counterfactual using the control group.
:::
