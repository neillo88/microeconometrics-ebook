---
editor: 
  markdown: 
    wrap: 72
---

# Experiments: Regular Assignment

## Key Definitions

### Classical Randomized Experiment

::: {.callout-note icon="false"}
### Def. Classical Randomized Experiment

A randomized experiment that is both,

\par

1.  individualistic

    \par

2.  unconfounded[^1]
:::

[^1]: First used in Rubin, D.B. (1990) 'Formal Mode of Statistical
    Inference for Causal Effects' Journal of Statistical Planning and
    Inference, 25(3), pp. 279-292

-   individualistic: probability of selection depends on the
    characteristics of the individual, **alone**
-   unconfounded: treatment is independent on potential outcomes,
    conditional on characteristics.

### Unconfoundedness

Let $W$ be a $N Ã— 1$ vector denoting the treatment assignment of each
individual in the sample.

::: {.callout-note icon="false"}
### Def. Unconfounded Assignment (Version 1)

$$
\mathbf{Pr}(W|X,Y(0),Y(1))=\mathbf{Pr}(W|X, Y'(0),Y'(1))
$$ for all $W$ , $X$ ($N\times k$ matrix of $k$-covariates), Y(0), Y(1),
Y'(0), and Y'(1).
:::

-   Assignment is independent of potential outcomes.
-   This statement is equivalent to, $$
      \mathbf{Pr}(W|X, Y'(0),Y'(1))=\mathbf{Pr}(W|X,Y(0),Y(1))
      $$

### Completely Randomized Experiment

A fixed, pre-determined number of subjects is assigned to receive the
treatment.

$$
\mathbf{Pr}(W|X,Y(0),Y(1))=
\begin{cases}
(\frac{N!}{N_t!(N-N_t)!})^{-1} & \text{if} \space\space \Sigma^N_{i=1} W_i = N_t\\
0 & \text{otherwise}
\end{cases}
$$

::: {.callout-note icon="false"}
### Def. Completely Randomized Experiment

A classical randomized experiment in which the number of units assigned
to treatment is fixed according to,

$$
1 \leq N_t \leq N - 1
$$
:::

### Propensity Score

::: {.callout-note icon="false"}
### Def. Finite Population Propensity Score

The propensity score at $x$ is the average unit assignment probability
for units with $X_i = x$ $$
e(x)=\frac{1}{N(x)} \sum_{i=1} p_i(X,Y(0),Y(1))
$$
:::

::: {.callout-note icon="false"}
### Def. Unit Assignment Probability

The probability that unit $i$ is assigned to treatment, $$
p_i(X,Y(0),Y(1)) = \sum_{W:W_i=1}\mathbf{Pr}(W|X,Y(0),Y(1))
$$
:::

In a completely randomized experiment, $$
e(X_i)=\frac{N_t}{N}
$$ i.e. equal for all units and independent of $X_i$

### Stratified Randomized Experiment

Stratification involves the dividing of the population into **blocks**
or **strata** ($B_i \in \{1,...,J\}$), based on *pre-treatment*,
observable characteristics $X_i$: $B_i = B_i(X_i)$

\par

-   Completely randomized experiment within each block and assignment is
    independent across blocks.

::: {.callout-note icon="false"}
### Def. Stratified Randomized Experiment

A stratified randomized experiment with $J$ blocks satisfying, $$
\mathbf{Pr}(W|X,Y(0),Y(1))=
\begin{cases}
\prod^J_{j=1} (\frac{N(j)!}{N(j)_t!(N(j)-N(j)_t)!})^{-1} & \text{if} \quad \sum^N_{i:B_i=j} W_i=N(j)_t \\
                    0 &\text{otherwise}
\end{cases}
$$ and \$N(j)\_c is preset such that, $$
0 < N(j)_t <N(j) \quad \text{for} \space j = 1,...,J
$$
:::

### Unconfoundedness

::: {.callout-note icon="false"}
### Def. Independent Assignment

$$Y(1),Y(0)\perp W$$
:::

-   Randomization is the **assignment mechanism** that gives you
    independence.
-   If you stratify based on characteristics X we might generalize the
    notion of independence to *unconfoundedness*

::: {.callout-note icon="false"}
### Def. Unconfounded Assignmet (Version 2)

$$
Y(1),Y(0) \perp W|X
$$
:::

This matches Angrist and Pischke's definition of the Conditional
Independence Assumption (MM and MHE; week 3)

## Why randomize?

### Selection

The difference between means gives us, $$
\begin{aligned}
E[Y_i|W_i=1] - E[Y_i|W_i=0]\\
=E[Y_i(1)|W_i=1]- E[Y_i(0)|W_i=0] \\
=\underbrace{E[Y_i(1)|W_i=1]-E[Y_i(0)|W_i=1]}_{\tau_{ATT}=E[Y_i(1)-Y_i(0)|W_i=1]} & + \underbrace{E[Y_i(0)|W_i=1]- E[Y_i(0)|W_i=0]}_{\text{Selection}}
\end{aligned}
$$ What is the source of selection bias?

\par

-   Individuals may select into treatment.
    -   e.g. job training for unemployed workers.
    -   e.g. maternity leave.
-   Treatment may be assigned according to a predefined grouping.
    -   e.g. minimum wage set at the state level.
    -   e.g. means tested public policy.

### Why randomize?

Unconfoundedness gives us $$
E\left[Y_i(0)|W_i=1\right]=E\left[Y_i(0)|W_i=0\right]=E\left[Y_i(0)\right]
$$ implying no selection **in expectation**

$$
\underbrace{E\left[Y_i(0)|W_i=1\right]-E\left[Y_i(0)|W_i=0\right]}_\text{Selection}=0 
$$
The observed difference \textbf{identifies} the ATT. 
	\begin{equation*}		
		E\left[Y_i|W_i=1\right]-E\left[Y_i|W_i=0\right]=\tau_{att} 
	\end{equation*}
	and 
	\begin{equation*}
		\tau_{ATT}=\tau_{ATU}=\tau_{ATE}
	\end{equation*}
	Randomization makes the comparison of two samples *ceteris paribus*   **in expectation**.

### Limits of randomization
Recall, \par
-   Randomization allows us to learn about $f_{Y(1)}$ and $f_{Y(0)}$
-   Randomization **does not** allow us to learn about $f_{Y(1),Y(0)}$ or $f_{Y(1)-Y(0)}$ 

Example from Banerjee *et al.* (2015, p. 38) [^2]
![](Images/Banerjee_p38.png)

[^2]:Banerjee, A., Duflo, E., Glennerster, R. & Kinnan, C. 2015, "The Miracle of Microfinance? Evidence from a Randomized Evaluation", American economic journal. Applied economics, vol. 7, no. 1, pp. 22-53.

### Randomization in practice

In Applied Economics, policy evaluations that embrace randomization are commonly referred to as **Randomized Control Trials (RCTs)**. \par
-   These are typically the gold standard. 
-   Unfortunately, randomization is not always feasible, it can be unethical, and RCTs are generally very expensive.
    -   Rely on the design and implementation of policies to exploit other research designs: difference-in-differences, regression discontinuity designs.
-   Useful lens through which to approach a research question? 
    -   For example, what is the impact of attending an Ivy-league school?
    -   The ideal experiment: randomize university allocation across all applicants.
    -   How far is another research design from this ideal?

## Hypothesis Testing: Fisher vs Neyman

### The Relevant Hypothesis for Finite Sample

When evaluating **finite samples**, there are two potential null hypothesis:
1.    Fisher's sharp null hypothesis:
			$$
			\begin{align*}
				&H_0: Y_i(1)=Y_i(0) \quad\forall i=1,...,N \\
				\text{against }&H_1:\;\exists\;i\; \text{s.t.}\;Y_i(1)\neq Y_i(0)
			\end{align*}
			$$
2.    Neyman's (finite sample) average treatment effect (ATE) hypothesis:
			$$
			\begin{align*}
				&H'_0: \frac{1}{N}\sum_{i=1}^{N}(Y_i(1)-Y_i(0))=0 \\
				\text{against }&H'_1:\frac{1}{N}\sum_{i=1}^{N}(Y_i(1)-Y_i(0))\neq0
		\end{align*}
		  $$

### Fisher's exact p-values

Consider the test statistic,
$$
T(W,Y^{obs}) = \bar{Y}^{obs}_t-\bar{Y}^{obs}_c
$$

Since we know the assignment mechanism under randomization, we can consider other realizations of W,
		$$
			T(\tilde{W},Y^{obs})
		$$
**under the null hypothesis.**\par
-   **Why?** Under $H_0$, $Y^{obs}=Y(1)=Y(0)$ 
-   Just need to consider other allocations of $W$
-   In fact, we can calculate the \textit{exact} distribution of $T$ under the null. 
-   Can calculate \textit{exact} p-value
-   Works for many test-statistics, including rank
		
Suppose we observe the data on outcomes $Y^{obs}$ and treatment status $W$, 

|Y^{obs}|W|
|--|--|
|	3 |  1|  
| 6 |  0 |
|	9 |  1 |
|	4 |  1 |
|	5 |  0  |
|	2 |  0  |

### Fisher: How does this work?

Depending on treatment status, we either observe $Y_i(0)$ or $Y_i(1)$,

$Y^{obs}$ | $W$ | $Y(0)$ | $Y(1)$ 
-|-|-|-
				3 |  1 | - | 3 
				6 |  0 | 6 | - 
				9 |  1 |-  |9 
				4 |  1 | - | 4 
				5 |  0 | 5 | - 
				2 |  0 | 2 | 

With this data we can compute any test-static $T(W,Y^{obs})$. For example, the standard t-statistic (assuming unequal variances),

$$
T(W,Y^{obs}) = \frac{\hat{\tau}}{\hat{se}(\hat{\tau})} = \frac{\bar{Y}^{obs}_t-\bar{Y}^{obs}_c}{\sqrt{\frac{S^2_c}{N_c}+\frac{S^2_t}{N_t}}}
$$
Since the null hypothesis - $H_0: Y_i(0)=Y_i(1) \quad\forall\; i=1,...,N$ - **is sharp**, we know, 

$Y^{obs}$ | $W$ | $Y(0)$ | $Y(1)$ 
-|-|-|-
				3 |  1 |  <span style="color: red">3</span> | 3 
				6 |  0 | 6 | <span style="color: red">6</span> 
				9 |  1 |<span style="color: red">9</span> | 9 
				4 |  1 | <span style="color: red">4</span> | 4 
				5 |  0 | 5 | <span style="color: red">5</span> 
				2 |  0 | 2 | <span style="color: red">2</span>											

**under the null**. 
		
We can therefore construct other realizations of the treatment allocation: $W'$.		
		
$Y^{obs}$ | $W$ | $Y(0)$ | $Y(1)$ | <span style="color: blue">$W'$</span> | <span style="color: blue">$Y^{obs'}$</span>
-|-|-|-|-|-
			3 |  1 |  <span style="color: red">3</span> | 3| <span style="color: blue">0</span> | <span style="color: blue">3</span> 
			6 |  0 | 6 | <span style="color: red">6</span> | <span style="color: blue">1</span> | <span style="color: blue">6</span> 
				9 |  1 |<span style="color: red">9</span> | 9  | <span style="color: blue">1</span> | <span style="color: blue">9</span> 
		4 |  1 | <span style="color: red">4</span> | 4 | <span style="color: blue">0</span> | <span style="color: blue">4</span> 
				5 |  0 | 5 | <span style="color: red">5</span> | <span style="color: blue">0</span> | <span style="color: blue">5</span> 
				2 |  0 | 2 | <span style="color: red">2</span> |  <span style="color: blue">1</span> | <span style="color: blue">2</span>		

$$
T(W',Y(1),Y(0)) 
$$
Compute for **all** possible realizations of $W$, get the exact distribution of the test static **under the sharp null hypothesis**. 


### Neyman's ATE test

Neyman's null hypothesis is, \par

$$
H_0:\;\tau^{fs}_{ATE} = 0 \qquad\text{against}\qquad H_0:\;\tau^{fs}_{ATE} \neq 0
$$

We can express,
	
$$
		\tau^{fs}_{ATE} = \frac{1}{N}\sum_{i=1}^{N}(Y_i(1)-Y_i(0))=\bar{Y}(1)-\bar{Y}(0)
$$
	
	Then, 
	
$$
		\hat{\tau} = \bar{Y}^{obs}_t-\bar{Y}^{obs}_c
$$
	
Is an **unbiased** estimator for $\tau^{fs}_{ATE}$. 
	

<div style="text-align: center;">PROOF [ON BOARD]</div>

::: {.callout-important collapse="true" icon="false"}
## Proofs
$$
\begin{aligned}
\hat{\tau} = &\bar{Y}^{obs}_t-\bar{Y}^{obs}_c \\
=&\frac{1}{N_t}\sum_{i:w_i=1}y_i-\frac{1}{N_c}\sum_{i:w_0=1}y_i \\
=&\frac{1}{N_t}\sum_{i=1}w_iy_i-\frac{1}{N_c}\sum_{i=0}(1-w_i)y_i \\
=&\frac{1}{N}[\sum_{i=1}^{N}\frac{w_iy_i}{N_t/N}-\sum_{i=0}^{N}\frac{(1-w_i)y_i}{N_c/N}]
\end{aligned}
$$
Neyman Test statisitc -> $\mathbf{E}_w[\hat{\tau}|y(1),y(0)]=\tau_{ATE}^{fs}$

where $\mathbf[E]_w$ means taking expectation with respect to $w$

$$
\begin{aligned}
\mathbf{E}_w[\hat{\tau}|y(1),y(0)] = &\frac{1}{N}[\sum_{i=1}^{N}\frac{\mathbf{E}_w[w_i]y_i}{N_t/N}-\sum_{i=0}^{N}\frac{\mathbf{E}_w[1-w_i]y_i}{N_c/N}] \\
 where &\space \mathbf{E}_w[w_i]=\frac{N_t}{N} \space\space\space\space\space \mathbf{E}_w[1-w_i]=\frac{N_c}{N} \\
=&\frac{1}{N}[\sum_{i=1}^{N}\frac{(N_t/N)y_i}{N_t/N}-\sum_{i=0}^{N}\frac{(N_c/N)y_i}{N_c/N}] \\
=&\frac{1}{N}\sum_{i=1}^{N}(y_i(1)-y_i(0))
\end{aligned}
$$
:::

### Sampling Variance of Neyman estimator

Remember, with a fixed sample, the source of variation is $W$.

-   $E_W[W_i]=E_W[W_i^2] = \frac{N_t}{N}$
-   $Var_W(W_i) = \frac{N_t}{N}\cdot(1-\frac{N_t}{N})$

But we also need to consider the correlation between $W_i$ and $W_{i'}$:

-   $E_W[W_iW_{i'}]=Pr_W(W_i=1)\cdot Pr(W_i=1|W_i=1)  = \frac{N_t}{N}\cdot\frac{N_t-1}{N-1}$ for $i\neq i'$
		
::: {.callout-tip icon="false"}

#### Finite sampling variance of $\hat{\tau}$
$$
				V^{fs}(\hat{\tau}) = \frac{S^2_c}{N_c}+\frac{S^2_t}{N_t}-\frac{S^2_{ct}}{N}
$$
:::

-   The proof is a little involved!

$$
		S^2_c = \frac{1}{N-1}\sum_{i=1}^N (Y_i(0)-\bar{Y}(0))^2
$$

$$
	S^2_t = \frac{1}{N-1}\sum_{i=1}^N (Y_i(1)-\bar{Y}(1))^2
$$
**Note, the variance of $Y(1)$ and $Y(0)$ need not be the same. And,
$$
		\begin{align*}
		S^2_{ct} &= \frac{1}{N-1}\sum_{i=1}^N (Y_i(1)-Y_i(0)-(\bar{Y}(1)-\bar{Y}(0)))^2 \\
		&=\frac{1}{N-1}\sum_{i=1}^N (Y_i(1)-Y_i(0)-\tau^{fs}_{ATE})^2
		\end{align*}
$$
	
-   Third term captures the sample variation in the unit-level TE.
-   If TE are constant (i.e. *homogenous*), then equals 0. 

	We have no way of estimating the $S^2_{ct}$ since it requires observation of both $Y_i(1)$ and $Y_i(0)$ for the same unit.
		
::: {.callout-important icon="false"}

#### Theorem: Neyman's variance estimator

If the treatment effect is constant, then an unbiased estimator for the sampling variance is,
$$
		\hat{V}^{neyman} = \frac{s^2_c}{N_c}+\frac{s^2_t}{N_t}
$$
:::
where, 
$$
		s^2_c = \frac{1}{N_c-1}\sum_{i:W_i=0}^N (Y_i^{obs}-\bar{Y}^{obs}_c)^2
$$
$$
		s^2_t = \frac{1}{N_t-1}\sum_{i:W_i=1}^N (Y_i^{obs}-\bar{Y}^{obs}_t)^2
$$	
** Note, this is different to the T-test variance






### Sampling Variance of Neyman estimator: An improvement

However, we can improve on this estimator for small samples. If we assume that $Y_i(1)-Y_i(0)$ is a constant, then
$$
S^2 = S^2_c=S^2_t
$$

::: {.callout-important icon="false"}
## Constant variance estimator
If the treatment effect is constant, then a more precise, unbiased estimator for the sampling variance is,
$$
			\hat{V}^{const} = s^2\cdot\left(\frac{1}{N_c}+\frac{1}{N_t}\right)
$$
:::
	where, 
$$
		s^2 = \frac{1}{N-2}\cdot\left(s^2_c\cdot(N_c-1)+s^2_t\cdot(N_t-1)\right)
$$
	** These equations should be fairly familiar.



### Neyman hypothesis test
To conduct inference we need to know the distribution of the test-statistic, not just the variance.

Potential approaches:

1.    Use a normal approximation of the randomization distribution of $\hat{\tau}$
2.    Approximate distribution of $\hat{V}$ by chi-squared and use t-distribution. 
$$
			t = \frac{\hat{\tau}}{\sqrt{\hat{V}}}
$$
	
Confidence intervals,
$$
	CI^{0.95}(\tau^{fs}_{ATE}) \approxeq \left[\hat{\tau}+z_{0.025}\cdot\sqrt{\hat{V}},\hat{\tau}+z_{0.975}\cdot\sqrt{\hat{V}}\right]
$$	
or 
$$
		CI^{0.95}(\tau^{fs}_{ATE}) \approxeq \left[\hat{\tau}+t_{N-2,0.025}\cdot\sqrt{\hat{V}},\hat{\tau}+t_{N-2,0.975}\cdot\sqrt{\hat{V}}\right]
$$



### Fisher vs Neyman
Remember the differences in the null hypotheses:\par
-   Fisher [**exact**]: $H_0: Y_i(1)=Y_i(0) \quad\forall i=1,...,N$
-   Neyman: $H'_0: \frac{1}{N}\sum_{i=1}^{N}(Y_i(1)-Y_i(0))=0$

From Imbens and Wooldridge (2009, p.23)[^3],

[^3]: Imbens, G.W. & Wooldridge, J.M. 2009, "Recent Developments in the Econometrics of Program Evaluation", Journal of economic literature, vol. 47, no. 1, pp. 5-86.

![](Images/table1_imbenswooldridge2009.png)
	
### Neyman: Super-population equivalent
Suppose we think of the sample N as a \textbf{random} draw from a potentially infinite super-population,
		
::: {.callout-important icon="false"}
## Result
$$
				E[\hat{\tau}] = E[\tau^{fs}_{ATE}] = \tau_{ATE}
$$
$$
				V(\hat{\tau}) = E\left[(\bar{Y}^{obs}_t-\bar{Y}^{obs}_c-E[\bar{Y}^{obs}_t-\bar{Y}^{obs}_c])^2\right]= \frac{\sigma^2_c}{N_c}+\frac{\sigma^2_t}{N_t}
$$
		Proof will be provided on Moodle.
:::
-   $\hat{V}^{neyman}$ is an unbiased estimator for $V$, **even with heterogeneous TE's**.
-   $\hat{V}^{neyman}$ is a better default choice.

		

<div style="text-align: center;">VARIANCE PROOF ON MOODLE </div>


## Balance tests

### Testing for balance in covariates
		
All Randomized Control Trials should provide some variant of a balance table: a table comparing the means of pre-treatment covariates in each of the treatment groups. 

For example,  (Oreopoulus, 2011, p.158)[^4]

[^4]: Oreopoulos, P. 2011, "Why Do Skilled Immigrants Struggle in the Labor Market? A Field Experiment with Thirteen Thousand Resumes", American economic journal. Economic policy, vol. 3, no. 4, pp. 148-171.

![](Images/Oreopoulus_Balance_1.png)
	


These tests are typically based on the ANOVA (<span style ="color : blue">AN</span>alysis <span style = "color: blue">O</span>f <span style ="color : blue">VA</span>riance) test.[^5]

[^5]: Common Alternatives include Multivariate ANOVA (MANOVA) and Analysis of Covariance (ANCOVA)

-   Decomposes TSS into,[^6]

[^6]: Check your EC124 notes on this.

$$
				TSS = \sum_{k=1}^{K}\sum_{i=1}^{N(k)}(X_{ik}-\bar{X})^2 = \underbrace{\sum_{k=1}^{K}\sum_{i=1}^{N(k)}(X_{ik}-\bar{X}_k)^2}_\text{residual/within}+\underbrace{\sum_{k=1}^{K}N(k)(\bar{X}_k-\bar{X})^2}_\text{explained/between}
$$

-   The F-statistic, 
$$
				F-stat =\frac{ESS/(K-1)}{RSS/(N-K)} = \frac{\text{explained/between variance}}{\text{residual/within variance}}
$$



This is to estimating the linear regression model, 
	
$$
			X_i = \beta_0 + \beta_1D_{i1}+...+ \beta_KD_{i,K-1}+\varepsilon_i
$$
		
where the dummy variables indicate $K-1$ groups. Test the joint linear hypothesis,
$$
			H_0: \beta_1 = ... = \beta_{K-1} = 0
$$
		
against, 
$$
			H_1: \text{at least one }\beta_j\neq0\qquad j=1,...,K-1
$$
		
-   Note, you should be wary of testing each $\beta$ coefficient separately. **Why?**
		
For example, Dupas and Robinson (2013, p.1150)[^7]

[^7]: Dupas, P. & Robinson, J. 2013, "Why Don't the Poor Save More? Evidence from Health Savings Experiments", The American economic review, vol. 103, no. 4, pp. 1138-1171.

![](Images/Dupas_Baseline_1.png)

It is important to remember, \par
-   Balance tests provide support for unconfoundedness (i.e. randomization has worked)
-   We can test for differences in observables.
-   We cannot rule out selection on unobservables. 
-   Partially rely on sufficiently large sample size. 

### Next-up
-   Evaluating RCT's with Linear Regression and (one of) Angus Deaton's critiques
-   Covariates, Efficiency, and Power		
-   Good and Bad Controls
-   Intent to Treat and some examples


	
<!---





here starts the second slide









--->

## Evaluating RCTs using a Linear Regression Model

### Linear Regression Model
Consider the linear regression *model*, 
$$
			Y_i = \alpha+\beta W_i + \varepsilon_i
$$
where, intuitively, $Y_i = Y^{obs}_i$. The OLS estimator for the vector of parameters $(\alpha,\beta)$ is defined as, 
$$
			(\hat{\alpha}^{OLS},\hat{\beta}^{OLS}) = \underset{a,b}{argmin} \sum_{i=1}^{N}(Y_i-a-b W_i)^2
$$
We know that in the case of a simple linear regression model, 
$$
			\hat{\beta}^{OLS} = \frac{\sum_{i=1}^{N}(W_i-\bar{W})(Y_i-\bar{Y})}{\sum_{i=1}^{N}(W_i-\bar{W})^2} = (W'M_\ell W)^{-1}W'M_\ell Y
$$
	
the sample analogue of $\beta = \frac{Cov(W_i,Y_i)}{Var(W_i)}$.
		
Since $\bar{W}=\frac{N_t}{N}$, we can show that [^8]

[^8]: Do ensure that you can show this.

$$
			\hat{\beta}^{OLS} = \bar{Y}^{obs}_t-\bar{Y}^{obs}_c = \hat{\tau}
$$
	
Which we know is an unbiased estimator for both $\tau_{ATE}^{fs}$ and $\tau_{ATE}$. \par
-   $\hat{\beta}^{OLS}$ is an unbiased estimate of the (average) causal effect, assuming unconfoundedness (i.e. completely random experiment).

We want to create a mapping from potential outcomes to a linear regression model. Recall, 
$$
			Y^{obs}_i = Y_i(0)+W_i(Y_i(1)-Y_(0))
$$
Let's define[^9],

[^9]: Here I am avoiding finite sample notation. This is not a big issue as you can always define the expectation as an average in the finite sample.

$$
		\begin{align*}
			\mu &=E[Y_i(0)]	\\
			\tau_{ATE} &= E[Y_i(1)-Y_i(0)]
		\end{align*}
$$		
We want to show that we can express $Y^{obs}_i$ as a linear regression model,
$$
			Y^{obs}_i=\mu+\tau_{ATE}W_i+\varepsilon_i
$$

### Endogeneity

We write the error term from the linear regression model as function of these parameters and the potential outcomes in the following way,
$$
		\begin{align*}
			\varepsilon_i 	&=Y_i(0)- \mu+W_i\cdot(Y_i(1)-Y_i(0)-\tau_{ATE}) \\
							&=\begin{cases*}
								Y^{obs}_i-\mu \qquad\qquad\quad \text{if}\quad W_i=0 \\
								Y^{obs}_i-\mu-\tau_{ATE} \qquad \text{if}\quad W_i=1
							\end{cases*} \\
							&=\begin{cases*}
								Y_i(0)-E[Y_i(0)] \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad \text{if}\quad W_i=0 \\
								Y_i(0)-E[Y_i(0)]+\underbrace{\left(Y_i(1)-Y_i(0)-E[Y_i(1)-Y_i(0)]\right)}_\text{heterogeneity} \quad \text{if}\quad W_i=1
							\end{cases*} 			 	 		
		\end{align*}
$$
Note, for a more straight forward mapping we can assume homogenous TEs,
$$
			(Y_i(1)-Y_i(0)) = \tau\qquad \forall i
$$
		
$$
\begin{align}
			Y_i &= Y_i(0)+W_i\cdot\underbrace{(Y_i(1)-Y_i(0))}_{\tau} \\
				&=E[Y_i(0)]+\tau W_i+\underbrace{Y_i(0)-E[Y_i(0)]}_{\varepsilon_i} \\
				&=\mu+\tau W_i + \varepsilon_i
\end{align}
$$
As an assignment mechanism, randomization gives us unconfoundedness,
$$
			(Y_i(1),Y_i(0))\perp W_i
$$
So, then we can consider the conditional expectations of $\varepsilon_i$,\par
-   **$W_i=0$**
$$
			\begin{align*}
				E[\varepsilon_i|W_i=0] &= E[Y_i(0)- \mu + W_i\cdot(Y_i(1)-Y_i(0)-\tau_{ATE})|W_i=0] \\
				&= E[Y_i(0)- \mu|W_i=0] \\
				&=E[Y_i(0)|W_i=0]-E[Y_i(0)] = 0
			\end{align*}
$$
-   **$W_i=1$**
$$
			\begin{align*}
				E[\varepsilon_i|W_i=1] &= E[Y_i(0)- \mu + W_i\cdot(Y_i(1)-Y_i(0)-\tau_{ATE})|W_i=1] \\
				&= E[Y_i(1)-\tau_{ATE} - \mu|W_i=1] \\
				&=E[Y_i(1)|W_i=1]-E[Y_i(0)]-E[Y_i(1)-Y_i(0)] = 0
			\end{align*}
$$
Unconfoundedness gives you mean independence.

### Inference
Suppose we assume homoskedasticity: $\sigma_i^2=\sigma^2$  
$$
		\begin{align*}
			\hat{\sigma}^2 = \frac{1}{N-2}\sum_{i=1}^N \hat{\varepsilon}_i^2&= \frac{1}{N-2}\sum_{i=1}^N  (Y_i-\hat{\alpha}^{OLS}-\hat{\beta}^{OLS}W_i)^2 \\
			&=\frac{1}{N-2}\sum_{i=1}^N  \left(Y_i-\bar{Y}-(\bar{Y}^{obs}_t-\bar{Y}^{obs}_c)\cdot(W_i-\bar{W})\right)^2 \\
			&=\frac{1}{N-2}\left(\sum_{i:W_i=0}^N(Y_i-\bar{Y}^{obs}_c)^2+\sum_{i:W_i=1}^N(Y_i-\bar{Y}^{obs}_t)^2\right) \\
			&=s^2
		\end{align*}
$$

where, 
$$
		s^2 = \frac{1}{N-2}\cdot\left(s^2_c\cdot(N_c-1)+s^2_t\cdot(N_t-1)\right)
$$
** Check to see if you can show this. 

### Deaton's critique

Angus Deaton has made a number of criticisms against the use of RCTs in (Development) Economics. One of these is the misuse of linear regression:[^10]

[^10]: Deaton, A. 2010, "Instruments, Randomization, and Learning about Development", Journal of economic literature, vol. 48, no. 2, pp. 424-455.

::: {.callout-tip icon="false"}

## Deaton (2010, p.442)
"However, the standard error of $\beta_1$ from	the OLS regression is not generally correct. One problem is that the variance among the experimentals may be different from the variance among the controls, and to assume that the experiment \textit{does not} affect the variance is very much against the minimalist spirit of RCTs."
:::

The standard OLS variance estimator for $\hat{\beta}^{OLS}$ is,
$$
		\hat{V}^{homosk} = \frac{\hat{\sigma}^2}{\sum_{i=1}^{N}(W_i-\bar{W})^2}=s^2\cdot\left(\frac{1}{N_c}+\frac{1}{N_t}\right)=\hat{V}^{const}
$$
If instead you estimate heteroskedastic standard errors,\footnote{Some texts scale the heteroskedasticity-robust variance estimator by $N/(N-2)$ (e.g., Stock and Watson, 2020, p.180). With or without the rescaling, you can show that this equality is not exact, given the formulae for $s^2_c$ and $s^2_t$ from lecture 2.1. In this case, you can show that $\hat{V}^{hetero}= \frac{\tilde{s}^2_c}{N_c}+\frac{\tilde{s}^2_t}{N_t}$, where $\tilde{s}^2_c$ and $\tilde{s}^2_t$ are scaled by $N_c$ and $N_t$, respectively. Not by $N_c-1$ and $N_t-1$.} 
$$
			\hat{V}^{hetero} = \frac{\sum_{i=1}^{N}\hat{\varepsilon}_i^2\cdot(W_i-\bar{W})^2}{\left(\sum_{i=1}^{N}(W_i-\bar{W})^2\right)^2} \approxeq \frac{s^2_c}{N_c}+\frac{s^2_t}{N_t}=\hat{V}^{neyman}
$$
	
::: {.callout-tip icon="false"}
## Deaton (2010, p.442)

"If the regression ... is run with the standard heteroskedasticity correction to the standard error, the result will be the same as the formula for the standard error of the difference between two means,	but not otherwise except in the special case where there are equal numbers of experimental and controls, in which case it turns out that the correction makes no difference	and the OLS standard error is correct. It is not clear in the experimental development literature whether the correction is routinely	done in practice..."
:::

## Adding covariates

### Adding covariates

Let us consider the linear regression model, 
$$
			Y_i = \alpha+\beta W_i + X_i'\gamma+\epsilon_i
$$
	
where, \par
-   $W_i$ is assigned according to a completely random experiment
-   $X_i$ is a $k\times 1$ vector of **pre-treatment** covariates
-   the linear function **need not** be correctly specified
	
Note: we are not interested in the interpretation of the $\gamma$'s or $\alpha$.







::: {.callout-important icon="false"}
### Theorem (Imbens and Rubin, 2015, p.123)
Suppose we conduct a **completely randomized experiment** in a sample drawn at random from an infinite population. Then, \par
1.    $plim\;\hat{\beta}=\tau_{ATE}$
2.    The asymptotic distribution of $\hat{\beta}$ is normal.
:::
This is a powerful result! \par
-   We are agnostic about the Conditional Expectation Function: $E[Y_i|W_i,X_i']$.
-   And the model need not be correctly specified.


### Small sample bias
Unfortunately, in small samples $\hat{\beta}^{OLS}$ is a biased estimator of $\tau_{ATE}$.[^11]

[^11]: Focus on the intuition of the results, not the equation. Taken from Deaton's (2010, p.444) discussion of Freedman (2008). In this equation $X_i$ is single covariate.

-   The bias is $\psi/N$ and $\psi$ is given by,
$$
				\psi = -\lim \frac{1}{N}\sum_{i=1}^N (\tau_i-\tau_{ATE})f(X_i)
$$
-   The source of the bias lies in the correlation between the unit-level treatment effect and the covariates.
-   Recall, treatment is independent of the covariates.
-   But, the individual level treatment effect may not be.

	
::: {.callout-tip icon="false"}
## In practice
Include covariates and be wary if the estimated coefficient changes substantially.
:::

### Efficiency gains
With covariates, the sampling variance of the OLS estimator is reduced. Here are the estimators:[^12]

[^12]:Subject of first seminar.

$$
\hat{V}^{hetero} = \frac{1}{N(N-k)}\cdot\frac{\sum_{i=1}^{N}(W_i-\bar{W})^2(Y_i-\hat{\alpha}^{OLS}-\hat{\beta}^{OLS}-X_i'\hat{\gamma}^{OLS})^2}{\left(\bar{W}(1-\bar{W})\right)^2}
$$ 
or 
$$
			\hat{V}^{homosk} = \frac{1}{N(N-k)}\cdot\frac{\sum_{i=1}^{N}(Y_i-\hat{\alpha}^{OLS}-\hat{\beta}^{OLS}-X_i'\hat{\gamma}^{OLS})^2}{\bar{W}(1-\bar{W})}
$$
where $k$ is the number of parameters estimated (including the constant). \par
-   Note, this holds only for **good covariates**

		
### Good controls
The question of good and bad controls is **very** important.[^13]

[^13]: See Q7 from 2022 exam for as exercise.
	
::: {.callout-note icon="false"}
##  Good control
A covariate that is not an outcome of the experiment.
:::

-   Variables measured at baseline,
-   or individual characteristics that cannot change with the experiment (e.g. age).

What about stratification? \par
-   You should include stratifying variables in the model
-   Stratification improves precision.


### Minimizing Variance
Note, in the variance formulae we have the term in the denominator, 
$$
			\sum_{i=1}^N(W_i-\bar{W})^2 = N\bar{W}(1-\bar{W})
$$

<div style = "text-align : center">
		TRY TO SHOW THIS
</div>
Which comes from the sample variance of the treatment variable. \par
	
To reduce the variance, we can maximize this function. This yields the solution,
	
$$
		\bar{W}^*=0.5
$$

-   **Equal assignment is efficient.**




### Under-powered
RCTs often face the risk that they are **under-powered**. 
::: {.allout-note icon="false"}
## Power
The probability of rejecting the null hypothesis ($H_0: \tau=0$; i.e., of no effect) when it is **false**.
$$
				\theta(\tau_0)=Pr(\text{Reject }H_0|\tau=\tau_0)
$$
:::
		
Recall, that we reject $H_0: \tau=0$ when [^13]

[^13]: Here I am being more general over the choice of the estimator $\hat{\tau}$ and I am approximating the distribution of the test-statistic with the standard normal.

$$
			|S|>z_{1-\alpha/2}\Longleftrightarrow |\hat{\tau}/se(\hat{\tau})|>z_{1-\alpha/2}
$$
Where $S$ is the test-statistic.
		

$$
		\begin{align*}
			\theta(\tau_0)=&Pr(|\hat{\tau}/se(\hat{\tau})|>z_{1-\alpha/2}|\tau=\tau_0) \\
			=&Pr(\hat{\tau}/se(\hat{\tau})<z_{\alpha/2}|\tau=\tau_0)+Pr(\hat{\tau}/se(\hat{\tau})>z_{1-\alpha/2}|\tau=\tau_0) \\
			=&Pr\left(\frac{\hat{\tau}-\tau_0}{se(\hat{\tau})}<z_{\alpha/2}-\frac{\tau_0}{se(\hat{\tau})}|\tau=\tau_0\right)\\
			&+Pr\left(\frac{\hat{\tau}-\tau_0}{se(\hat{\tau})}>z_{1-\alpha/2}-\frac{\tau_0}{se(\hat{\tau})}|\tau=\tau_0\right) \\
			=&Pr\left(Z<z_{\alpha/2}-\frac{\tau_0}{se(\hat{\tau})}|\tau=\tau_0\right)+Pr\left(Z>z_{1-\alpha/2}-\frac{\tau_0}{se(\hat{\tau})}|\tau=\tau_0\right)
		\end{align*}
$$		
Given the shape of the normal distribution, $\theta(\tau_0)$ is increasing in $\tau_0/se(\hat{\tau})$: \par
-    Power is increasing in the absolute value of $\tau_0$
-   Power decreasing in the $Var(\hat{\tau})\Rightarrow$ power is **increasing in N**.

## The Microfinance `Miracle': A Case of ITT

### The Microfinance `Miracle'
'<span style="color: blue"> The Miracle of Microfinance? Evidence from a Randomized Evaluation[^2]</span>' \par
		
-   Microfinance was seen as a `silver bullet' for fighting poverty, but robust evidence was scarce. 
-   Existing evidence based on self-selected clients - not causal. 
-   <span style="color: blue">Setting:</span> Hyderabad, India, capital of Andrha Pradesh
-   <span style="color: blue">Sample:</span> randomly treat 52 (out of 104) neighbourhoods with new Spandana branch.
-   <span style="color: blue">Data Collection:</span> survey households 18 months, and then 42 months after treatment (N=6,850). 
-   <span style="color: blue">Research Design:</span> ``focus here on reduced-form/intent-to-treat estimates'' because of spillover or general equilibrium effects.


*"given the sampling frame, ours will be an intent-to-treat (ITT) analysis on	a sample of "likely borrowers." This is thus neither the effect on those who borrow nor the average effect on the neighborhood. Rather, it is the average effect of easier access to microfinance on those who are its primary targets.'' (p.35)*
		
**Intention-To-Treat (ITT)** \par
-   Based on initial random assignment (i.e. reduced form)
-   Not on take-up or receipt of treatment

	

*"The main drawback of these ITT analyses is that they do not answer questions about causal effects of the receipt of treatment itself, only about causal effects of the assignment to treatment." (Imbens and Rubin, 2015, p.515)[^16]*

[^16]: More generally used in discussion of non-compliance.
		
![](Images/Banerjee_Timeline_1.png)


	
-    Outcomes of interest:
1.    consumption
2.    business creation
3.    business income
4.    education
5.    health
6.    women's empowerment
 
-   Initial (18-month) take-up is lower than expected (by Spandana): 26.7% in treated neighbourhoods and 18.3% in control.
-   NOTE: control group is not `untreated' as other companies enter both treated and control groups during this period.\par
    -   By second survey both groups had similar take-up, but treatment group had bigger and older loans. 
	
-   Highlights after 18 months: \par
    1.    Decline in informal borrowing.
		2.    No difference in borrowed amount.
		3.    No difference in (nondurable) consumption, but increase in purchase of durable goods.
		4.    No impact on entrepreneurship, but invest in existing businesses.
		5.    Increase in profitability of some businesses (upper tail)
			
-   Highlights after 42 months:\par
		1.    Businesses have more assets, more businesses are more profitable
		2.    Average still small $\Rightarrow$ doesn't help small businesses
		3.     No evidence of consumption change

-   No effect on female empowerment outcomes. 
-   Almost 70% of households do not have an MFI loan, despite eligibility, yet borrow from other sources.

	
![](Images/Banerjee_Baseline_1.png)


<p style="font-size: 14px;">NOTE: Baseline survey covers a different set of households.</p>
	
The authors estimate the following regression model in each period after the intervention,[^17]

[^17]: I have use the authors' notation.
		
$$
			y_{ia} = \alpha + \beta \times Treat_{ia} + X_a'\gamma + \varepsilon_{ia}
$$
		
where $i$ represents the individual and $a$ the area (neighbourhood). \par
-   $X_a$ is a vector of area controls \textbf{at baseline}, including population, per capita literature, etc. 

	
![](Images/Banerjee_Regression_1.png)
![](Images/Banerjee_Regression_3.png)
![](Images/Banerjee_Regression_2.png)
	
Conclusion:

*"microcredit is not for every household, or even most households, and **it does not lead	to the miraculous social transformation** some proponents have claimed...allows some households to sacrifice some instantaneous utility (temptation goods or leisure) in order to finance	lumpy purchases, either for their home or in order to establish or expand a business"*
		
Similar results found in other RCTs, including ``Estimating the impact of microcredit on those who take it up: Evidence from a randomized experiment in Morocco'' by Cr\'{e}pon, Devoto, Duflo \& Parient\'{e} (2015) \par
-   Clearer research design with untreated control group
-   No evidence of a gain in income or consumption

	
<!---

simulation of the small sample variance

--->
#### Notes on sample variance (in R)

```{r}
data1 <-data.frame(y0 = c(1,3,2,4),
y1 = c(3,1,3,3)
)
data1$tau <-  data1$y1-data1$y0
data1$St <- c(sd(data1$y1)^2,sd(data1$y0)^2,sd(data1$tau)^2,NA)
print(paste("Finite Sample Variance Tau-hat:", data1$St[1]/2+data1$St[2]/2-data1$St[3]/4))
```

Code out all the possible realizations of W

```{r}
m <- 1
for (i in 1:3){
  j <- i + 1
  while (j <= 4) {
    data1[paste0("w", m)] <- as.numeric((1:nrow(data1) == i) | (1:nrow(data1) == j))
    m <- m + 1
    j <- j + 1
  }
}

tauhat <- matrix(NA, nrow = 6, ncol = 1)

for (i in 1:6) {
  data1[paste0("yobs", i)] <- data1$y1 * data1[paste0("w", i)] + data1$y0 * (1 - data1[paste0("w", i)])
  model <- lm(paste0("yobs", i, " ~ ", "w", i), data = data1)
  tauhat[i, 1] <- coef(model)[paste0("w", i)]
}
print(paste0("Finite Sample Variance Tau-hat is ", sd(tauhat)^2*5/6))
```
